{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import selenium\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# import bs4\n",
    "\n",
    "# d_path = r\"/Users/jack/utils/chromedriver\"\n",
    "# s = Service(d_path)\n",
    "# driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# url = \"https://news.naver.com/main/ranking/popularDay.naver\"\n",
    "# driver.get(url)\n",
    "# soup = bs4.BeautifulSoup(driver.page_source,features='html.parser')\n",
    "\n",
    "# ranking_news = soup.find('div', class_=\"_officeCard _officeCard0\").find_all('div', class_='rankingnews_box')\n",
    "\n",
    "# for news in ranking_news:\n",
    "#     news_name = news.find('strong', class_='rankingnews_name').get_text(strip=True)\n",
    "#     print(news_name)\n",
    "#     for i, titles in enumerate(news.find_all('a', class_=\"list_title nclicks('RBP.rnknws')\")):\n",
    "#         print(f\"{i + 1} : \", titles.get_text(strip=True))\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---20221025----\n",
      "한겨레\n",
      "경향신문\n",
      "한국경제TV\n",
      "KBS\n",
      "매일경제\n",
      "SBS\n",
      "머니투데이\n",
      "중앙일보\n",
      "헤럴드경제\n",
      "한국경제\n",
      "YTN\n",
      "아시아경제\n",
      "---20221024----\n",
      "조선일보\n",
      "헤럴드경제\n",
      "YTN\n",
      "머니투데이\n",
      "MBC\n",
      "연합뉴스TV\n",
      "JTBC\n",
      "SBS\n",
      "뉴스1\n",
      "조선비즈\n",
      "국민일보\n",
      "KBS\n",
      "---20221023----\n",
      "아시아경제\n",
      "JTBC\n",
      "YTN\n",
      "서울경제\n",
      "SBS\n",
      "헤럴드경제\n",
      "한국경제\n",
      "MBC\n",
      "한겨레\n",
      "문화일보\n",
      "중앙일보\n",
      "이데일리\n",
      "---20221022----\n",
      "SBS\n",
      "중앙일보\n",
      "연합뉴스\n",
      "이데일리\n",
      "한국경제\n",
      "매일경제\n",
      "MBC\n",
      "JTBC\n",
      "YTN\n",
      "경향신문\n",
      "머니투데이\n",
      "한겨레\n",
      "---20221021----\n",
      "MBC\n",
      "노컷뉴스\n",
      "국민일보\n",
      "파이낸셜뉴스\n",
      "SBS\n",
      "연합뉴스\n",
      "매일경제\n",
      "아시아경제\n",
      "중앙일보\n",
      "조선일보\n",
      "문화일보\n",
      "한겨레\n",
      "---20221020----\n",
      "한국경제\n",
      "JTBC\n",
      "조선일보\n",
      "연합뉴스TV\n",
      "서울신문\n",
      "아시아경제\n",
      "중앙일보\n",
      "세계일보\n",
      "국민일보\n",
      "뉴스1\n",
      "이데일리\n",
      "연합뉴스\n",
      "---20221019----\n",
      "YTN\n",
      "KBS\n",
      "이데일리\n",
      "JTBC\n",
      "한겨레\n",
      "아시아경제\n",
      "한국일보\n",
      "한국경제\n",
      "MBC\n",
      "머니투데이\n",
      "중앙일보\n",
      "SBS\n",
      "---20221018----\n",
      "SBS\n",
      "연합뉴스\n",
      "머니투데이\n",
      "MBC\n",
      "중앙일보\n",
      "JTBC\n",
      "YTN\n",
      "국민일보\n",
      "조선비즈\n",
      "한겨레\n",
      "서울경제\n",
      "뉴스타파\n",
      "---20221017----\n",
      "YTN\n",
      "서울신문\n",
      "조선일보\n",
      "머니투데이\n",
      "매일경제\n",
      "전자신문\n",
      "아시아경제\n",
      "이데일리\n",
      "조선비즈\n",
      "경향신문\n",
      "한국경제TV\n",
      "연합뉴스\n",
      "---20221016----\n",
      "JTBC\n",
      "한국일보\n",
      "한겨레\n",
      "노컷뉴스\n",
      "KBS\n",
      "YTN\n",
      "아시아경제\n",
      "한국경제\n",
      "중앙일보\n",
      "연합뉴스\n",
      "SBS\n",
      "조선비즈\n",
      "---20221015----\n",
      "조선비즈\n",
      "헤럴드경제\n",
      "미디어오늘\n",
      "조선일보\n",
      "국민일보\n",
      "경향신문\n",
      "전자신문\n",
      "중앙일보\n",
      "서울경제\n",
      "노컷뉴스\n",
      "연합뉴스\n",
      "KBS\n",
      "---20221014----\n",
      "서울경제\n",
      "한국경제\n",
      "KBS\n",
      "중앙일보\n",
      "이데일리\n",
      "YTN\n",
      "한국일보\n",
      "머니투데이\n",
      "강원일보\n",
      "디지털데일리\n",
      "조선비즈\n",
      "세계일보\n",
      "---20221013----\n",
      "헤럴드경제\n",
      "JTBC\n",
      "MBN\n",
      "한국경제\n",
      "SBS\n",
      "국민일보\n",
      "조선일보\n",
      "중앙일보\n",
      "매일경제\n",
      "오마이뉴스\n",
      "연합뉴스\n",
      "MBC\n",
      "---20221012----\n",
      "뉴시스\n",
      "아시아경제\n",
      "한겨레\n",
      "중앙일보\n",
      "연합뉴스\n",
      "조선일보\n",
      "서울경제\n",
      "JTBC\n",
      "머니투데이\n",
      "국민일보\n",
      "조선비즈\n",
      "KBS\n",
      "---20221011----\n",
      "헤럴드경제\n",
      "매일경제\n",
      "연합뉴스\n",
      "JTBC\n",
      "노컷뉴스\n",
      "디지털타임스\n",
      "조선일보\n",
      "아시아경제\n",
      "강원일보\n",
      "한국경제\n",
      "파이낸셜뉴스\n",
      "국민일보\n",
      "---20221010----\n",
      "경향신문\n",
      "KBS\n",
      "MBC\n",
      "중앙일보\n",
      "매일경제\n",
      "YTN\n",
      "조선일보\n",
      "MBN\n",
      "조선비즈\n",
      "연합뉴스\n",
      "JTBC\n",
      "TV조선\n",
      "---20221009----\n",
      "조선일보\n",
      "조선비즈\n",
      "MBC\n",
      "동아일보\n",
      "SBS\n",
      "뉴시스\n",
      "연합뉴스TV\n",
      "중앙일보\n",
      "경향신문\n",
      "파이낸셜뉴스\n",
      "한국경제\n",
      "한국일보\n",
      "---20221008----\n",
      "JTBC\n",
      "서울신문\n",
      "조선비즈\n",
      "조선일보\n",
      "미디어오늘\n",
      "중앙일보\n",
      "뉴스타파\n",
      "KBS\n",
      "매일경제\n",
      "아시아경제\n",
      "머니투데이\n",
      "데일리안\n",
      "---20221007----\n",
      "조선일보\n",
      "디지털타임스\n",
      "TV조선\n",
      "한국경제TV\n",
      "뉴스1\n",
      "YTN\n",
      "한겨레\n",
      "매일경제\n",
      "아시아경제\n",
      "중앙일보\n",
      "이데일리\n",
      "전자신문\n",
      "---20221006----\n",
      "아시아경제\n",
      "한국경제\n",
      "YTN\n",
      "KBS\n",
      "더팩트\n",
      "동아일보\n",
      "노컷뉴스\n",
      "시사IN\n",
      "조선일보\n",
      "뉴스1\n",
      "부산일보\n",
      "헤럴드경제\n",
      "---20221005----\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m _date\u001b[39m=\u001b[39mdriver\u001b[39m.\u001b[39mcurrent_url[\u001b[39m-\u001b[39m\u001b[39m8\u001b[39m:]\n\u001b[1;32m     29\u001b[0m soup \u001b[39m=\u001b[39m bs4\u001b[39m.\u001b[39mBeautifulSoup(driver\u001b[39m.\u001b[39mpage_source,features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m ranking_news \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_officeCard _officeCard0\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrankingnews_box\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m news \u001b[39min\u001b[39;00m ranking_news:\n\u001b[1;32m     32\u001b[0m     news_name \u001b[39m=\u001b[39m news\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mstrong\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrankingnews_name\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget_text(strip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import bs4\n",
    "import pickle\n",
    "\n",
    "d_path = r\"/Users/jack/utils/chromedriver\"\n",
    "s = Service(d_path)\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = \"https://news.naver.com/main/ranking/popularDay.naver\"\n",
    "driver.get(url)\n",
    "soup = bs4.BeautifulSoup(driver.page_source,features='html.parser')\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        next_page_xpath = '//*[@id=\"wrap\"]/div[4]/ul/li[4]/a'\n",
    "        driver.find_element(by=By.XPATH,value=next_page_xpath).click()\n",
    "        time.sleep(2)\n",
    "        print(f\"---{driver.current_url[-8:]}----\")\n",
    "    except:\n",
    "        break\n",
    "    _date=driver.current_url[-8:]\n",
    "\n",
    "    soup = bs4.BeautifulSoup(driver.page_source,features='html.parser')\n",
    "    ranking_news = soup.find('div', class_=\"_officeCard _officeCard0\").find_all('div', class_='rankingnews_box')\n",
    "    for news in ranking_news:\n",
    "        news_name = news.find('strong', class_='rankingnews_name').get_text(strip=True)\n",
    "        print(news_name)\n",
    "        _temp = list()\n",
    "        for titles in news.find_all('a', class_=\"list_title nclicks('RBP.rnknws')\"):\n",
    "            _temp.append(titles.get_text(strip=True))\n",
    "            # print(titles.get_text(strip=True))\n",
    "        with open(f\"news_crawling/{news_name}_{_date}.pickle\",'wb') as f:\n",
    "            pickle.dump(_temp,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---20220610----\n",
      "JTBC\n",
      "한겨레\n",
      "MBC\n",
      "KBS\n",
      "동아일보\n",
      "서울경제\n",
      "이데일리\n",
      "한국경제\n",
      "헤럴드경제\n",
      "YTN\n",
      "중앙일보\n",
      "서울신문\n",
      "---20220609----\n",
      "한국경제\n",
      "YTN\n",
      "서울경제\n",
      "아시아경제\n",
      "SBS\n",
      "미디어오늘\n",
      "국민일보\n",
      "조선일보\n",
      "중앙일보\n",
      "데일리안\n",
      "KBS\n",
      "문화일보\n",
      "---20220608----\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m _date\u001b[39m=\u001b[39mdriver\u001b[39m.\u001b[39mcurrent_url[\u001b[39m-\u001b[39m\u001b[39m8\u001b[39m:]\n\u001b[1;32m     16\u001b[0m soup \u001b[39m=\u001b[39m bs4\u001b[39m.\u001b[39mBeautifulSoup(driver\u001b[39m.\u001b[39mpage_source,features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m ranking_news \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_officeCard _officeCard0\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrankingnews_box\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m news \u001b[39min\u001b[39;00m ranking_news:\n\u001b[1;32m     19\u001b[0m     news_name \u001b[39m=\u001b[39m news\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mstrong\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrankingnews_name\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget_text(strip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\"\"\"https://news.naver.com/main/ranking/popularDay.naver?date=20220611\"\"\"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # driver.find_element(by=By.XPATH,value='//*[@id=\"footer\"]/address/a[1]').send_keys(Keys.PAGE_DOWN)\n",
    "        next_page_xpath = '//*[@id=\"wrap\"]/div[4]/ul/li[4]/a'\n",
    "        driver.find_element(by=By.XPATH,value=next_page_xpath).click()\n",
    "        time.sleep(1.3)\n",
    "        print(f\"---{driver.current_url[-8:]}----\")\n",
    "    except:\n",
    "        break\n",
    "    _date=driver.current_url[-8:]\n",
    "\n",
    "    soup = bs4.BeautifulSoup(driver.page_source,features='html.parser')\n",
    "    ranking_news = soup.find('div', class_=\"_officeCard _officeCard0\").find_all('div', class_='rankingnews_box')\n",
    "    for news in ranking_news:\n",
    "        news_name = news.find('strong', class_='rankingnews_name').get_text(strip=True)\n",
    "        print(news_name)\n",
    "        _temp = list()\n",
    "        for titles in news.find_all('a', class_=\"list_title nclicks('RBP.rnknws')\"):\n",
    "            _temp.append(titles.get_text(strip=True))\n",
    "            # print(titles.get_text(strip=True))\n",
    "        with open(f\"news_crawling/{news_name}_{_date}.pickle\",'wb') as f:\n",
    "            pickle.dump(_temp,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import bs4\n",
    "import pickle\n",
    "\n",
    "d_path = r\"/Users/jack/utils/chromedriver\"\n",
    "s = Service(d_path)\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/main/ranking/popularDay.naver?date=20220518\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---20220607----\n",
      "조선일보\n",
      "한국경제TV\n",
      "한겨레\n",
      "경향신문\n",
      "한국경제\n",
      "KBS\n",
      "미디어오늘\n",
      "한국일보\n",
      "뉴스1\n",
      "중앙일보\n",
      "조선비즈\n",
      "SBS\n",
      "---20220606----\n",
      "뉴시스\n",
      "조선비즈\n",
      "디지털타임스\n",
      "중앙일보\n",
      "미디어오늘\n",
      "조선일보\n",
      "동아일보\n",
      "한경비즈니스\n",
      "JTBC\n",
      "SBS\n",
      "한국경제\n",
      "이데일리\n",
      "---20220605----\n",
      "한국일보\n",
      "연합뉴스TV\n",
      "파이낸셜뉴스\n",
      "머니투데이\n",
      "아시아경제\n",
      "헤럴드경제\n",
      "이데일리\n",
      "한국경제\n",
      "서울경제\n",
      "중앙일보\n",
      "한겨레\n",
      "SBS\n",
      "---20220604----\n",
      "MBC\n",
      "한국경제TV\n",
      "중앙일보\n",
      "뉴스1\n",
      "아시아경제\n",
      "MBN\n",
      "한국경제\n",
      "경향신문\n",
      "SBS\n",
      "TV조선\n",
      "JTBC\n",
      "국민일보\n",
      "---20220603----\n",
      "한국경제\n",
      "노컷뉴스\n",
      "서울신문\n",
      "동아일보\n",
      "중앙일보\n",
      "SBS\n",
      "디지털타임스\n",
      "경향신문\n",
      "서울경제\n",
      "오마이뉴스\n",
      "MBC\n",
      "조선비즈\n",
      "---20220602----\n",
      "조선일보\n",
      "헤럴드경제\n",
      "문화일보\n",
      "조선비즈\n",
      "MBN\n",
      "YTN\n",
      "머니투데이\n",
      "JTBC\n",
      "부산일보\n",
      "아시아경제\n",
      "중앙일보\n",
      "KBS\n",
      "---20220601----\n",
      "---20220601----\n",
      "한국일보\n",
      "노컷뉴스\n",
      "서울신문\n",
      "한국경제\n",
      "문화일보\n",
      "프레시안\n",
      "중앙일보\n",
      "매일경제\n",
      "KBS\n",
      "조선일보\n",
      "연합뉴스TV\n",
      "한국경제TV\n",
      "---20220531----\n",
      "서울신문\n",
      "조선일보\n",
      "헤럴드경제\n",
      "SBS\n",
      "한국일보\n",
      "매일경제\n",
      "이데일리\n",
      "중앙일보\n",
      "연합뉴스TV\n",
      "JTBC\n",
      "KBS\n",
      "뉴시스\n",
      "---20220530----\n",
      "JTBC\n",
      "한국일보\n",
      "조선비즈\n",
      "YTN\n",
      "ZDNet Korea\n",
      "헤럴드경제\n",
      "연합뉴스\n",
      "한국경제TV\n",
      "강원일보\n",
      "경향신문\n",
      "조선일보\n",
      "파이낸셜뉴스\n",
      "---20220529----\n",
      "MBC\n",
      "파이낸셜뉴스\n",
      "조선비즈\n",
      "서울경제\n",
      "KBS\n",
      "한겨레\n",
      "JTBC\n",
      "아시아경제\n",
      "국민일보\n",
      "헤럴드경제\n",
      "한국경제\n",
      "아이뉴스24\n",
      "---20220528----\n",
      "MBC\n",
      "매일경제\n",
      "이데일리\n",
      "SBS\n",
      "아이뉴스24\n",
      "JTBC\n",
      "한국경제\n",
      "YTN\n",
      "중앙일보\n",
      "머니투데이\n",
      "조선비즈\n",
      "연합뉴스\n",
      "---20220527----\n",
      "서울신문\n",
      "미디어오늘\n",
      "이데일리\n",
      "MBC\n",
      "문화일보\n",
      "YTN\n",
      "블로터\n",
      "한국경제\n",
      "채널A\n",
      "중앙일보\n",
      "한국경제TV\n",
      "조선비즈\n",
      "---20220526----\n",
      "---20220526----\n",
      "한국경제TV\n",
      "세계일보\n",
      "이데일리\n",
      "아시아경제\n",
      "뉴스1\n",
      "조선비즈\n",
      "중앙일보\n",
      "YTN\n",
      "블로터\n",
      "국민일보\n",
      "조선일보\n",
      "디지털타임스\n",
      "---20220525----\n",
      "---20220525----\n",
      "뉴스타파\n",
      "한국경제\n",
      "오마이뉴스\n",
      "서울경제\n",
      "매일경제\n",
      "서울신문\n",
      "MBN\n",
      "JTBC\n",
      "국민일보\n",
      "한겨레21\n",
      "조선일보\n",
      "경향신문\n",
      "---20220524----\n",
      "---20220524----\n",
      "연합뉴스TV\n",
      "조선일보\n",
      "문화일보\n",
      "국민일보\n",
      "연합뉴스\n",
      "동아일보\n",
      "한겨레\n",
      "매일경제\n",
      "KBS\n",
      "한국경제\n",
      "서울신문\n",
      "아시아경제\n",
      "---20220523----\n",
      "조선일보\n",
      "KBS\n",
      "한국일보\n",
      "연합뉴스\n",
      "아시아경제\n",
      "조선비즈\n",
      "MBN\n",
      "매일경제\n",
      "SBS\n",
      "한국경제\n",
      "JTBC\n",
      "MBC\n",
      "---20220522----\n",
      "매일경제\n",
      "중앙일보\n",
      "파이낸셜뉴스\n",
      "한국경제\n",
      "노컷뉴스\n",
      "서울신문\n",
      "MBC\n",
      "서울경제\n",
      "KBS\n",
      "조선일보\n",
      "뉴시스\n",
      "YTN\n",
      "---20220521----\n",
      "조선일보\n",
      "JTBC\n",
      "YTN\n",
      "SBS\n",
      "아시아경제\n",
      "이데일리\n",
      "한국경제TV\n",
      "머니투데이\n",
      "헤럴드경제\n",
      "동아일보\n",
      "연합뉴스\n",
      "MBN\n",
      "---20220520----\n",
      "조선일보\n",
      "연합뉴스\n",
      "MBC\n",
      "KBS\n",
      "중앙일보\n",
      "경향신문\n",
      "한국일보\n",
      "헤럴드경제\n",
      "한국경제\n",
      "강원일보\n",
      "한국경제TV\n",
      "SBS\n",
      "---20220519----\n",
      "채널A\n",
      "JTBC\n",
      "서울경제\n",
      "한겨레\n",
      "매일경제\n",
      "중앙일보\n",
      "조선일보\n",
      "조선비즈\n",
      "MBN\n",
      "연합뉴스\n",
      "KBS\n",
      "한국일보\n",
      "---20220518----\n",
      "이데일리\n",
      "헤럴드경제\n",
      "중앙일보\n",
      "경향신문\n",
      "머니투데이\n",
      "SBS Biz\n",
      "MBC\n",
      "조선비즈\n",
      "한겨레\n",
      "한국일보\n",
      "서울경제\n",
      "KBS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        # driver.find_element(by=By.XPATH,value='//*[@id=\"footer\"]/address/a[1]').send_keys(Keys.PAGE_DOWN)\n",
    "        next_page_xpath = '//*[@id=\"wrap\"]/div[4]/ul/li[4]/a'\n",
    "        driver.find_element(by=By.XPATH,value=next_page_xpath).click()\n",
    "        time.sleep(1.3)\n",
    "        print(f\"---{driver.current_url[-8:]}----\")\n",
    "    except:\n",
    "        break\n",
    "    try:\n",
    "        _date=driver.current_url[-8:]\n",
    "\n",
    "        soup = bs4.BeautifulSoup(driver.page_source,features='html.parser')\n",
    "        ranking_news = soup.find('div', class_=\"_officeCard _officeCard0\").find_all('div', class_='rankingnews_box')\n",
    "        for news in ranking_news:\n",
    "            news_name = news.find('strong', class_='rankingnews_name').get_text(strip=True)\n",
    "            print(news_name)\n",
    "            _temp = list()\n",
    "            for titles in news.find_all('a', class_=\"list_title nclicks('RBP.rnknws')\"):\n",
    "                _temp.append(titles.get_text(strip=True))\n",
    "                # print(titles.get_text(strip=True))\n",
    "            with open(f\"news_crawling/{news_name}_{_date}.pickle\",'wb') as f:\n",
    "                pickle.dump(_temp,f)\n",
    "    except AttributeError:\n",
    "        driver.back()\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('crawling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10621530bc52bc3dcaea48e2b7ef028942e35d2e6e9c9234a88fefd069caf90f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
